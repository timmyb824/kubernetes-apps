fullnameOverride: prometheus

defaultRules:
  create: true
  rules:
    alertmanager: true
    etcd: true
    configReloaders: true
    general: true
    k8s: true
    kubeApiserverAvailability: true
    kubeApiserverBurnrate: true
    kubeApiserverHistogram: true
    kubeApiserverSlos: true
    kubelet: true
    kubeProxy: true
    kubePrometheusGeneral: true
    kubePrometheusNodeRecording: true
    kubernetesApps: true
    kubernetesResources: true
    kubernetesStorage: true
    kubernetesSystem: true
    kubeScheduler: true
    kubeStateMetrics: true
    network: true
    node: true
    nodeExporterAlerting: true
    nodeExporterRecording: true
    prometheus: true
    prometheusOperator: true

alertmanager:
  fullnameOverride: alertmanager
  enabled: true
  ingress:
    enabled: false
  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#alertmanagerspec
  ##
  ## Alertmanager configuration directives
  ## ref: https://prometheus.io/docs/alerting/configuration/#configuration-file
  ##      https://prometheus.io/webtools/alerting/routing-tree-editor/
  ##
  config:
    global:
      resolve_timeout: 5m
      # slack_api_url: ${SLACK_APP_URL}
    route:
      group_by: ['job']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      #receiver: 'slack'
      routes:
      - match:
          alertname: DeadMansSwitch
        receiver: 'null'
      - match:
          alertname: Watchdog
        receiver: 'null'
      - match:
          alertname: InfoInhibitor
        receiver: 'null'
      # - match:
      #   receiver: 'slack'
      #   continue: true
      - match:
        receiver: 'discord'
        continue: true
      - match:
        receiver: webhook
        continue: true
      # - match:
      #   receiver: 'alerta'
      #   continue: true
    receivers:
    - name: 'null'
    # - name: 'slack'
    #   slack_configs:
    #   - channel: '#alerts'
    #     send_resolved: true
    #     title: '[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] Monitoring Event Notification'
    #     text: >-
    #       {{ range .Alerts }}
    #         *Alert:* {{ .Annotations.summary }} - `{{ .Labels.severity }}`
    #         *Description:* {{ .Annotations.description }}
    #         *Graph:* <{{ .GeneratorURL }}|:chart_with_upwards_trend:>
    #         *Runbook:* <{{ .Annotations.runbook }}|:spiral_note_pad:>
    #         *Details:*
    #         {{ range .Labels.SortedPairs }} â€¢ *{{ .Name }}:* `{{ .Value }}`
    #         {{ end }}
    #       {{ end }}
    - name: 'webhook'
      webhook_configs:
      - url: 'http://homelab-oci03.local.lan:8435/'
        send_resolved: true
    # - name: "alerta"
    #   webhook_configs:
    #   - url: 'https://alerta.timmybtech.com/api/webhooks/prometheus?api-key=${ALERTA_API_KEY}'
    #     send_resolved: true
    - name: discord
      discord_configs:
      - webhook_url: ${DISCORD_WEBHOOK_URL}
        send_resolved: true
  alertmanagerSpec:
    retention: 120h
    ## Storage is the definition of how storage will be used by the Alertmanager instances.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
    ##
    storage:
    volumeClaimTemplate:
      spec:
        storageClassName: nfs-csi-synologynas
        accessModes: ["ReadWriteOnce"]
        resources:
          requests:
            storage: 10Gi

grafana:
  enabled: true
  sidecar:
    dashboards:
      enabled: true
      label: grafana_dashboard
      labelValue: "1"

      ## Annotations for Grafana dashboard configmaps
      ##
      annotations: {}
      multicluster:
        global:
          enabled: false
        etcd:
          enabled: false
      provider:
        allowUiUpdates: false
    datasources:
      enabled: true
      defaultDatasourceEnabled: true
      isDefaultDatasource: true
      uid: PROMETHEUS20231208
  additionalDataSources:
  - name: InfluxDB_Flux
    type: influxdb
    access: proxy
    uid: INFLUXDB20210108
    orgId: 1
    isDefault: false
    url: https://influxdb.local.timmybtech.com
    jsonData:
      version: Flux
      organization: homelab
      defaultBucket: proxmox
      tlsSkipVerify: true
    secureJsonData:
      token: '${INFLUXDB2_TOKEN}'

  - name: Zabbix
    type: alexanderzobnin-zabbix-datasource
    access: proxy
    url: http://192.168.86.203:8080/api_jsonrpc.php
    uid: ZABBIX20210108
    orgId: 1
    isDefault: false
    jsonData:
      # Zabbix API credentials
      username: ${ZUSER}
      password: ${ZPASSWORD}
      # Trends options
      trends: true
      trendsFrom: "7d"
      trendsRange: "4d"
      # Cache update interval
      cacheTTL: "1h"
      # Alerting options
      alerting: true
      addThresholds: false
      alertingMinSeverity: 3
      # Direct DB Connection options
      dbConnectionEnable: false
      # Name of existing datasource for Direct DB Connection
      dbConnectionDatasourceName:
      # Retention policy name (InfluxDB only) for fetching long-term stored data.
      # Leave it blank if only default retention policy used.
      dbConnectionRetentionPolicy: one_year
      # Disable acknowledges for read-only users
      disableReadOnlyUsersAck: true
      # Disable time series data alignment
      disableDataAlignment: false
      # Use value mapping from Zabbix
      useZabbixValueMapping: true
    version: 1
    editable: false

  - name: Loki
    type: loki
    access: proxy
    uid: LOKI20210108
    url: https://loki.local.timmybtech.com
    jsonData:
      maxLines: 1000

  fullnameOverride: grafana
  forceDeployDatasources: false
  forceDeployDashboards: false
  defaultDashboardsEnabled: true
  defaultDashboardsTimezone: est
  serviceMonitor:
    enabled: true
  admin:
    existingSecret: grafana-admin-credentials
    userKey: admin-user
    passwordKey: admin-password
  service:
    enabled: true
    type: ClusterIP
    port: 80
    targetPort: 3000
      # targetPort: 4181 To be used with a proxy extraContainer
    ## Service annotations. Can be templated.
    annotations: {}
    labels: {}
    portName: service
    # Adds the appProtocol field to the service. This allows to work with istio protocol selection. Ex: "http" or "tcp"
    appProtocol: ""
  plugins:
    - grafana-clock-panel
    - grafana-piechart-panel
    - alexanderzobnin-zabbix-app
    - https://github.com/performancecopilot/grafana-pcp/releases/download/v5.1.1/performancecopilot-pcp-app-5.1.1.zip;performancecopilot-pcp-app'
  persistence:
    type: pvc
    enabled: true
    storageClassName: nfs-csi-synologynas
    # commenting out will create new pvc
    # existingClaim: "grafana-pvc" #FIND
    accessModes:
      - ReadWriteOnce
    size: 20Gi
  securityContext:
    runAsGroup: 0
    fsGroup: 65534
    runAsNonRoot: true
    runAsUser: 472
  alerting:
    contactpoints.yaml:
      # apiVersion: 1
      contactPoints:
        - orgId: 1
          name: alerts-slack
          receivers:
            - uid: slack01
              type: slack
              settings:
                recipient: alerts
                url: ${SLACK_APP_URL}

kubeApiServer:
  enabled: true

kubelet:
  enabled: true
  serviceMonitor:
    metricRelabelings:
      - action: replace
        sourceLabels:
          - node
        targetLabel: instance
    ## Enable scraping /metrics/cadvisor from kubelet's service
    ##
    cAdvisor: true
    ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    cAdvisorMetricRelabelings:
      # Drop less useful container CPU metrics.
      - sourceLabels: [__name__]
        action: drop
        regex: 'container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)'
      # Drop less useful container / always zero filesystem metrics.
      - sourceLabels: [__name__]
        action: drop
        regex: 'container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)'
      # Drop less useful / always zero container memory metrics.
      - sourceLabels: [__name__]
        action: drop
        regex: 'container_memory_(mapped_file|swap)'
      # Drop less useful container process metrics.
      - sourceLabels: [__name__]
        action: drop
        regex: 'container_(file_descriptors|tasks_state|threads_max)'
      # Drop container spec metrics that overlap with kube-state-metrics.
      - sourceLabels: [__name__]
        action: drop
        regex: 'container_spec.*'
      # Drop cgroup metrics with no pod.
      - sourceLabels: [id, pod]
        action: drop
        regex: '.+;'
    # - sourceLabels: [__name__, image]
    #   separator: ;
    #   regex: container_([a-z_]+);
    #   replacement: $1
    #   action: drop
    # - sourceLabels: [__name__]
    #   separator: ;
    #   regex: container_(network_tcp_usage_total|network_udp_usage_total|tasks_state|cpu_load_average_10s)
    #   replacement: $1
    #   action: drop
    ## RelabelConfigs to apply to samples before scraping
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
    ##
    ## metrics_path is required to match upstream rules and charts
    cAdvisorRelabelings:
      - sourceLabels: [__metrics_path__]
        targetLabel: metrics_path
    # - sourceLabels: [__meta_kubernetes_pod_node_name]
    #   separator: ;
    #   regex: ^(.*)$
    #   targetLabel: nodename
    #   replacement: $1
    #   action: replace

kubeControllerManager:
  enabled: true
  endpoints: # ips of servers
    - 192.168.86.180
    - 192.168.86.185
    - 192.168.86.186

coreDns:
  enabled: true

kubeDns:
  enabled: false

kubeEtcd:
  enabled: true
  endpoints: # ips of servers
    - 192.168.86.180
    - 192.168.86.185
    - 192.168.86.186
  service:
    enabled: true
    port: 2381
    targetPort: 2381

kubeScheduler:
  enabled: true
  endpoints: # ips of servers
    - 192.168.86.180
    - 192.168.86.185
    - 192.168.86.186

kubeProxy:
  enabled: true
  endpoints: # ips of servers
    - 192.168.86.180
    - 192.168.86.185
    - 192.168.86.186

kubeStateMetrics:
  enabled: true

kube-state-metrics:
  fullnameOverride: kube-state-metrics
  selfMonitor:
    enabled: true
  prometheus:
    monitor:
      enabled: true
      relabelings:
        - action: replace
          regex: (.*)
          replacement: $1
          sourceLabels:
            - __meta_kubernetes_pod_node_name
          targetLabel: kubernetes_node

nodeExporter:
  enabled: true
  serviceMonitor:
    relabelings:
      - action: replace
        regex: (.*)
        replacement: $1
        sourceLabels:
          - __meta_kubernetes_pod_node_name
        targetLabel: kubernetes_node

prometheus-node-exporter:
  fullnameOverride: node-exporter
  podLabels:
    jobLabel: node-exporter
  extraArgs:
    - --collector.filesystem.mount-points-exclude=^/(dev|proc|sys|var/lib/docker/.+|var/lib/kubelet/.+)($|/)
    - --collector.filesystem.fs-types-exclude=^(autofs|binfmt_misc|bpf|cgroup2?|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|iso9660|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tracefs)$
  service:
    portName: http-metrics
  prometheus:
    monitor:
      enabled: true
      relabelings:
        - action: replace
          regex: (.*)
          replacement: $1
          sourceLabels:
            - __meta_kubernetes_pod_node_name
          targetLabel: kubernetes_node
  resources:
    requests:
      memory: 512Mi
      cpu: 250m
    limits:
      memory: 2048Mi

prometheusOperator:
  enabled: true
  prometheusConfigReloader:
    resources:
      requests:
        cpu: 200m
        memory: 50Mi
      limits:
        memory: 100Mi

prometheus:
  enabled: true
  prometheusSpec:
    resources:
      requests:
        cpu: 200m
        memory: 2000Mi
      limits:
        cpu: 500m
        memory: 3000Mi
    replicas: 1
    replicaExternalLabelName: "replica"
    ruleSelectorNilUsesHelmValues: false
    serviceMonitorSelectorNilUsesHelmValues: false
    podMonitorSelectorNilUsesHelmValues: false
    probeSelectorNilUsesHelmValues: false
    enableAdminAPI: true
    walCompression: true
  ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#prometheusspec
  ##
    ## Prometheus StorageSpec for persistent data
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/user-guides/storage.md
    ##
    ## Interval between consecutive scrapes.
    ## Defaults to 30s.
    ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/release-0.44/pkg/prometheus/promcfg.go#L180-L183
    ##
    scrapeInterval: "1m"
    ## Number of seconds to wait for target to respond before erroring
    ##
    scrapeTimeout: "55s"
    ## Interval between consecutive evaluations.
    ##
    evaluationInterval: "1m"
    ## How long to retain metrics
    ##
    retention: 10d
    storageSpec:
    ## Using PersistentVolumeClaim
    ##
      volumeClaimTemplate:
        spec:
          storageClassName: nfs-csi-synologynas
          volumeName: pvc-a70c5e20-a66e-453c-bcf5-78f6bfa74a1b
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 32Gi
    ##

    additionalScrapeConfigs:
      - job_name: prometheus-federation
        metrics_path: /federate
        scrape_interval: 1m
        scrape_timeout: 55s
        scheme: http
        honor_labels: true
        params:
          'match[]':
            - '{__name__=~".+"}'
        static_configs:
          - targets:
              - '${PROMETHEUS_OCI01}'
              # - '${PROMETHEUS_OCI02}'
              - '${PROMETHEUS_OCI03}'
              # - '192.168.86.42:9090'
              - '192.168.86.53:9090'
              - '192.168.86.74:9090'
              - '192.168.86.174:9090'
              - '192.168.86.201:9090'
              - '192.168.86.202:9090'
              - '192.168.86.203:9090'
              # - '192.168.86.132:9090'
              # - '192.168.86.59:9090'
              # - '${PROMETHEUS_LINODE01}'
              # - '${PROMETHEUS_AWS01}'
              # - '192.168.86.33:9090'

      # - job_name: caretta
      #   scrape_interval: 1m
      #   scrape_timeout: 55s
      #   metrics_path: /metrics
      #   static_configs:
      #     - targets:
      #         # - 'http://caretta-vm:8428'
      #         - http://prom-metrics:7117'
      #   relabel_configs:
      #   - action: keep
      #     source_labels: [__meta_kubernetes_pod_label_app]
      #     separator: ;
      #     regex: caretta
      #     replacement: $1
      #   - action: labelmap
      #     regex: __meta_kubrnetes_pod_label_(.+)
      #   - action: replace
      #     source_labels: [__meta_kubernetes_pod_name]
      #     target_label: caretta_pod
      #   - action: replace
      #     source_labels: [__meta_kubernetes_pod_node_name]
      #     target_label: caretta_node
      #   kubernetes_sd_configs:
      #   - role: pod
      #     namespaces:
      #       own_namespace: true
      #       names: []

      - job_name: 'traefik'
        scrape_interval: 1m
        scrape_timeout: 55s
        static_configs:
        - targets: ['traefik-metrics.local.timmybtech.com']

      - job_name: 'traefik-docker'
        scrape_interval: 1m
        scrape_timeout: 55s
        metrics_path: '/metrics'
        static_configs:
        - targets: ['traefik.timmybtech.com']

      - job_name: 'argocd'
        scrape_interval: 1m
        scrape_timeout: 55s
        metrics_path: '/metrics'
        static_configs:
          - targets: ['argocd-metrics.local.timmybtech.com']

      - job_name: 'gitea'
        scrape_interval: 1m
        scrape_timeout: 55s
        metrics_path: '/metrics'
        static_configs:
          - targets: ['gitea.local.timmybtech.com']

      - job_name: 'loki'
        scrape_interval: 1m
        scrape_timeout: 55s
        metrics_path: '/metrics'
        static_configs:
          - targets: ['loki-write.local.timmybtech.com']

      - job_name: 'healthchecks-${ZEMAIL}'
        scheme: https
        metrics_path: /projects/dd6ae2d4-92d3-49ef-aefb-747e0e8fc67a/metrics/${HEALTHCHECKS_API_KEY}
        static_configs:
        - targets: ['healthchecks.timmybtech.com']

      - job_name: minio-job
        bearer_token: ${MINIO_BEARER_TOKEN}
        metrics_path: /minio/v2/metrics/cluster
        scheme: https
        static_configs:
        - targets: [minio.local.timmybtech.com]

      # - job_name: alerta
      #   metrics_path: /api/management/metrics
      #   params:
      #     api-key: [${ALERTA_API_KEY}]
      #   scheme: https
      #   static_configs:
      #   - targets: [alerta.timmybtech.com]


      # - job_name: 'influxdb'
      #   scrape_interval: 1m
      #   scrape_timeout: 55s
      #   metrics_path: '/metrics'
      #   static_configs:
      #     - targets: ['influxdb.local.timmybtech.com']

      # - job_name: 'zincobserve'
      #   scrape_interval: 1m
      #   scrape_timeout: 55s
      #   metrics_path: '/metrics'
      #   static_configs:
      #     - targets: ['zincobserve.local.timmybtech.com']

      # - job_name: 'zinc'
      #   scrape_interval: 1m
      #   scrape_timeout: 55s
      #   metrics_path: '/metrics'
      #   static_configs:
      #     - targets: ['zinc.local.timmybtech.com']

      # - job_name: 'netdata-scrape'
      #   metrics_path: '/api/v1/allmetrics'
      #   params:
      #     # format: prometheus | prometheus_all_hosts
      #     # You can use `prometheus_all_hosts` if you want Prometheus to set the `instance` to your hostname instead of IP
      #     format: [prometheus_all_hosts]
      #     #
      #     # sources: as-collected | raw | average | sum | volume
      #     # default is: average
      #     #source: [as-collected]
      #     #
      #     # server name for this prometheus - the default is the client IP
      #     # for Netdata to uniquely identify it
      #     #server: ['prometheus1']
      #   honor_labels: true
      #   static_configs:
      #     - targets:
      #         - 192.168.86.220:19999

thanosRuler:
  enabled: false
